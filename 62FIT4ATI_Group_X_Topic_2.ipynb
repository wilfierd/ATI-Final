{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industrial Pump Predictive Maintenance using RNN\n",
    "\n",
    "**Course:** 62FIT4ATI - Artificial Intelligence\n",
    "\n",
    "**Topic 2:** Recurrent Neural Network for Predictive Maintenance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Problem Formulation\n",
    "\n",
    "### 1.1 Problem Overview\n",
    "\n",
    "**Predictive maintenance** is a proactive maintenance strategy that uses data analysis and machine learning to predict when equipment might fail, allowing maintenance to be scheduled before failures occur. This approach offers significant advantages over traditional reactive maintenance (fixing after failure) or preventive maintenance (scheduled maintenance regardless of condition).\n",
    "\n",
    "In this project, we develop a **Recurrent Neural Network (RNN)** model to predict the operational status of industrial pumps based on time-series sensor data. The goal is to classify the machine's status into one of three categories:\n",
    "\n",
    "| Class | Description |\n",
    "|-------|-------------|\n",
    "| **NORMAL** | The pump is operating within normal parameters |\n",
    "| **RECOVERING** | The pump is in a recovery state after an issue |\n",
    "| **BROKEN** | The pump has failed or is in a failure state |\n",
    "\n",
    "### 1.2 Why RNN for This Problem?\n",
    "\n",
    "Industrial sensor data is inherently **temporal** - the current state of a machine depends on its previous states. Traditional machine learning models treat each data point independently, losing valuable sequential information. RNNs, particularly **LSTM (Long Short-Term Memory)** networks, are designed to:\n",
    "\n",
    "1. **Capture temporal dependencies**: Learn patterns across time steps\n",
    "2. **Handle variable-length sequences**: Process sensor readings over different time windows\n",
    "3. **Remember long-term patterns**: Detect gradual degradation that precedes failures\n",
    "\n",
    "### 1.3 Key Challenges\n",
    "\n",
    "This project presents several significant challenges:\n",
    "\n",
    "#### Challenge 1: Extreme Class Imbalance\n",
    "\n",
    "The dataset exhibits severe class imbalance:\n",
    "- **NORMAL**: 205,836 samples (93.43%)\n",
    "- **RECOVERING**: 14,477 samples (6.57%)\n",
    "- **BROKEN**: 7 samples (0.003%)\n",
    "\n",
    "This imbalance means a naive model could achieve >93% accuracy by always predicting \"NORMAL\", while completely failing to detect actual failures. We address this through:\n",
    "- Class weighting during training\n",
    "- Focal loss function\n",
    "- Appropriate evaluation metrics (F1-score, macro-average)\n",
    "\n",
    "#### Challenge 2: Temporal Pattern Recognition\n",
    "\n",
    "Equipment failures often develop gradually through subtle changes in sensor readings. The model must learn to:\n",
    "- Identify early warning signs in sensor patterns\n",
    "- Distinguish between normal variations and anomalous trends\n",
    "- Capture both short-term fluctuations and long-term degradation\n",
    "\n",
    "#### Challenge 3: High-Dimensional Input\n",
    "\n",
    "With 52 sensor features, the model must handle high-dimensional input while avoiding overfitting. We employ:\n",
    "- Dropout regularization\n",
    "- Feature normalization\n",
    "- Appropriate model architecture\n",
    "\n",
    "### 1.4 Project Objectives\n",
    "\n",
    "1. Build an LSTM-based classifier for machine status prediction\n",
    "2. Implement techniques to handle extreme class imbalance\n",
    "3. Apply optimization techniques for stable RNN training\n",
    "4. Evaluate model performance with appropriate metrics\n",
    "5. Create an inference pipeline for new sensor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Identify Inputs and Outputs\n",
    "\n",
    "### 2.1 Dataset Overview\n",
    "\n",
    "The dataset contains time-series sensor readings from industrial pumps with the following characteristics:\n",
    "\n",
    "- **Total samples**: 220,320 time-series records\n",
    "- **Time period**: Continuous sensor readings at regular intervals\n",
    "- **Features**: 52 continuous sensor measurements + timestamp\n",
    "- **Target**: Machine operational status (3 classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Input Features (52 Sensors)\n",
    "\n",
    "The input consists of **52 continuous sensor measurements** (sensor_00 to sensor_51) that capture various physical properties of the industrial pump:\n",
    "\n",
    "| Feature Group | Sensors | Description |\n",
    "|--------------|---------|-------------|\n",
    "| sensor_00 - sensor_51 | 52 sensors | Continuous measurements including temperature, pressure, vibration, flow rate, and other operational parameters |\n",
    "\n",
    "Each sensor provides real-valued measurements that vary over time, capturing the operational state of the pump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Output Target (Machine Status)\n",
    "\n",
    "The target variable **machine_status** is a categorical variable with three possible values:\n",
    "\n",
    "| Class | Label | Description | Count | Percentage |\n",
    "|-------|-------|-------------|-------|------------|\n",
    "| 0 | NORMAL | Pump operating normally | 205,836 | 93.43% |\n",
    "| 1 | RECOVERING | Pump in recovery state | 14,477 | 6.57% |\n",
    "| 2 | BROKEN | Pump has failed | 7 | 0.003% |\n",
    "\n",
    "The model will output probability distributions over these three classes using softmax activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data Shapes and Types\n",
    "\n",
    "**Raw Data Shape:**\n",
    "- Input:  - 220,320 samples \u00d7 52 features\n",
    "- Target:  - 220,320 labels\n",
    "\n",
    "**After Sequence Creation (for RNN):**\n",
    "- Input:  - 3D tensor\n",
    "- Target:  - One-hot encoded labels\n",
    "\n",
    "**Data Types:**\n",
    "- Sensor features:  (continuous values)\n",
    "- Timestamp: \n",
    "- Machine status:  (categorical string)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Setup: Install dependencies and configure environment\n",
    "import sys\n",
    "\n",
    "# Check if running in Colab\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    \n",
    "    # Install required packages\n",
    "    !pip install -q hypothesis imbalanced-learn\n",
    "else:\n",
    "    print(\"Running locally - ensure dependencies are installed via requirements.txt\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option(\"display.max_columns\", 60)\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import our custom data loader\n",
    "import sys\n",
    "sys.path.insert(0, \"src\") if \"src\" not in sys.path else None\n",
    "\n",
    "from data_loader import load_csv, get_feature_columns, get_target_column, get_class_names\n",
    "\n",
    "# Display input/output specifications\n",
    "print(\"=\" * 50)\n",
    "print(\"INPUT FEATURES\")\n",
    "print(\"=\" * 50)\n",
    "feature_cols = get_feature_columns()\n",
    "print(f\"Number of sensor features: {len(feature_cols)}\")\n",
    "print(f\"Feature names: {feature_cols[:5]} ... {feature_cols[-3:]}\")\n",
    "\n",
    "print(\"\n\" + \"=\" * 50)\n",
    "print(\"OUTPUT TARGET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Target column: {get_target_column()}\")\n",
    "print(f\"Class names: {get_class_names()}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Data Preparation - Inspection\n",
    "\n",
    "In this section, we load and inspect the sensor dataset to understand its structure, identify data quality issues, and visualize key characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load and Display Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the sensor data\n",
    "# For Colab: Update path to your Google Drive location\n",
    "# For local: Use relative path\n",
    "\n",
    "DATA_PATH = \"sensor.csv\"  # Update this path as needed\n",
    "\n",
    "df = load_csv(DATA_PATH)\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(f\"\\nTotal samples: {len(df):,}\")\n",
    "print(f\"Total features: {len(df.columns)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Display data types\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"\\nDetailed column info:\")\n",
    "df.info()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Statistical summary of sensor features\n",
    "feature_cols = get_feature_columns()\n",
    "print(\"Statistical Summary of Sensor Features:\")\n",
    "df[feature_cols].describe()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Analyze class distribution\n",
    "target_col = get_target_column()\n",
    "class_counts = df[target_col].value_counts()\n",
    "class_percentages = df[target_col].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "for cls in class_counts.index:\n",
    "    count = class_counts[cls]\n",
    "    pct = class_percentages[cls]\n",
    "    print(f\"{cls:12s}: {count:>10,} samples ({pct:>6.3f}%)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total: {len(df):>16,} samples\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "colors = [\"#2ecc71\", \"#f39c12\", \"#e74c3c\"]\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(class_counts.index, class_counts.values, color=colors)\n",
    "ax1.set_xlabel(\"Machine Status\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_title(\"Class Distribution (Bar Chart)\")\n",
    "ax1.set_yscale(\"log\")  # Log scale due to extreme imbalance\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, class_counts.values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "             f\"{count:,}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "# Pie chart\n",
    "ax2 = axes[1]\n",
    "ax2.pie(class_counts.values, labels=class_counts.index, autopct=\"%1.2f%%\",\n",
    "        colors=colors, explode=[0, 0.05, 0.1])\n",
    "ax2.set_title(\"Class Distribution (Pie Chart)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u26a0\ufe0f CRITICAL: Extreme class imbalance detected!\")\n",
    "print(f\"   BROKEN class has only {class_counts.get(\"BROKEN\", 0)} samples ({class_percentages.get(\"BROKEN\", 0):.4f}%)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check for missing values\n",
    "missing_counts = df[feature_cols].isnull().sum()\n",
    "missing_pct = (missing_counts / len(df)) * 100\n",
    "\n",
    "# Create summary DataFrame\n",
    "missing_df = pd.DataFrame({\n",
    "    \"Missing Count\": missing_counts,\n",
    "    \"Missing %\": missing_pct\n",
    "}).sort_values(\"Missing Count\", ascending=False)\n",
    "\n",
    "# Show only columns with missing values\n",
    "missing_with_nulls = missing_df[missing_df[\"Missing Count\"] > 0]\n",
    "\n",
    "print(f\"Columns with missing values: {len(missing_with_nulls)} out of {len(feature_cols)}\")\n",
    "print(\"\\nMissing Values Summary:\")\n",
    "if len(missing_with_nulls) > 0:\n",
    "    print(missing_with_nulls)\n",
    "else:\n",
    "    print(\"No missing values found in sensor columns!\")\n",
    "\n",
    "print(f\"\\nTotal missing values: {missing_counts.sum():,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize missing values pattern\n",
    "if missing_counts.sum() > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    \n",
    "    # Show missing values heatmap for columns with missing data\n",
    "    cols_with_missing = missing_with_nulls.index.tolist()[:10]  # Top 10\n",
    "    if cols_with_missing:\n",
    "        sns.heatmap(df[cols_with_missing].isnull().T, cbar=True, \n",
    "                    yticklabels=True, cmap=\"YlOrRd\", ax=ax)\n",
    "        ax.set_title(\"Missing Values Pattern (Top 10 columns)\")\n",
    "        ax.set_xlabel(\"Sample Index\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No missing values to visualize.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Sensor Distributions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize distribution of selected sensors\n",
    "selected_sensors = [\"sensor_00\", \"sensor_10\", \"sensor_20\", \"sensor_30\", \"sensor_40\", \"sensor_50\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, sensor in enumerate(selected_sensors):\n",
    "    ax = axes[idx]\n",
    "    df[sensor].hist(bins=50, ax=ax, color=\"steelblue\", edgecolor=\"white\")\n",
    "    ax.set_title(f\"{sensor} Distribution\")\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.suptitle(\"Distribution of Selected Sensors\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Sensor Correlations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compute correlation matrix for sensor features\n",
    "# Using a subset for visualization clarity\n",
    "sensor_subset = feature_cols[:20]  # First 20 sensors\n",
    "\n",
    "corr_matrix = df[sensor_subset].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap=\"RdBu_r\", center=0,\n",
    "            square=True, linewidths=0.5, ax=ax)\n",
    "ax.set_title(\"Correlation Matrix (First 20 Sensors)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "high_corr_threshold = 0.9\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > high_corr_threshold:\n",
    "            high_corr_pairs.append((\n",
    "                corr_matrix.columns[i],\n",
    "                corr_matrix.columns[j],\n",
    "                corr_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "print(f\"\\nHighly correlated sensor pairs (|r| > {high_corr_threshold}):\")\n",
    "for s1, s2, corr in high_corr_pairs[:10]:\n",
    "    print(f\"  {s1} <-> {s2}: {corr:.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Time Series Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize sensor readings over time with machine status\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "# Sample a subset for visualization (every 100th point)\n",
    "sample_df = df.iloc[::100].copy()\n",
    "\n",
    "# Plot selected sensors\n",
    "sensors_to_plot = [\"sensor_00\", \"sensor_25\", \"sensor_50\"]\n",
    "\n",
    "for idx, sensor in enumerate(sensors_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Color by machine status\n",
    "    colors_map = {\"NORMAL\": \"green\", \"RECOVERING\": \"orange\", \"BROKEN\": \"red\"}\n",
    "    for status, color in colors_map.items():\n",
    "        mask = sample_df[\"machine_status\"] == status\n",
    "        ax.scatter(sample_df.index[mask], sample_df.loc[mask, sensor],\n",
    "                   c=color, label=status, alpha=0.5, s=1)\n",
    "    \n",
    "    ax.set_ylabel(sensor)\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "axes[-1].set_xlabel(\"Sample Index\")\n",
    "plt.suptitle(\"Sensor Readings Over Time (colored by Machine Status)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Class Imbalance Handling Analysis\n",
    "\n",
    "The extreme class imbalance in this dataset presents a critical challenge for model training. Without proper handling, the model would simply learn to predict the majority class (NORMAL) and achieve high accuracy while completely failing to detect actual failures.\n",
    "\n",
    "#### The Imbalance Problem\n",
    "\n",
    "| Class | Count | Percentage | Imbalance Ratio |\n",
    "|-------|-------|------------|----------------|\n",
    "| NORMAL | 205,836 | 93.43% | 1x (baseline) |\n",
    "| RECOVERING | 14,477 | 6.57% | ~14x underrepresented |\n",
    "| BROKEN | 7 | 0.003% | ~29,405x underrepresented |\n",
    "\n",
    "#### Techniques to Address Imbalance\n",
    "\n",
    "We employ two main techniques:\n",
    "\n",
    "1. **Class Weighting**: Assign higher weights to minority classes during training\n",
    "2. **Focal Loss**: A loss function that down-weights well-classified examples and focuses on hard cases"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import our imbalance handler module\n",
    "from imbalance_handler import compute_class_weights, get_class_distribution, compute_alpha_from_weights\n",
    "\n",
    "# Get class distribution statistics\n",
    "target_col = get_target_column()\n",
    "\n",
    "# Encode labels for weight computation\n",
    "from preprocessor import encode_labels\n",
    "labels_encoded, label_encoder = encode_labels(\n",
    "    df[target_col], \n",
    "    class_order=['NORMAL', 'RECOVERING', 'BROKEN']\n",
    ")\n",
    "\n",
    "# Compute class weights using inverse frequency\n",
    "class_weights = compute_class_weights(labels_encoded)\n",
    "\n",
    "print(\"Class Weights (Inverse Frequency):\")\n",
    "print(\"=\" * 60)\n",
    "class_names = ['NORMAL', 'RECOVERING', 'BROKEN']\n",
    "for idx, name in enumerate(class_names):\n",
    "    weight = class_weights[idx]\n",
    "    print(f\"{name:12s}: weight = {weight:>12.4f}\")\n",
    "print(\"=\" * 60)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize class weights\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart of class weights\n",
    "ax1 = axes[0]\n",
    "colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "bars = ax1.bar(class_names, [class_weights[i] for i in range(3)], color=colors)\n",
    "ax1.set_xlabel('Machine Status')\n",
    "ax1.set_ylabel('Class Weight')\n",
    "ax1.set_title('Class Weights (Inverse Frequency)')\n",
    "ax1.set_yscale('log')  # Log scale due to extreme differences\n",
    "\n",
    "# Add weight labels on bars\n",
    "for bar, idx in zip(bars, range(3)):\n",
    "    weight = class_weights[idx]\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "             f'{weight:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Pie chart showing effective contribution after weighting\n",
    "ax2 = axes[1]\n",
    "# After weighting, each class should contribute equally\n",
    "weighted_contributions = [1/3, 1/3, 1/3]  # Balanced after weighting\n",
    "ax2.pie(weighted_contributions, labels=class_names, autopct='%1.1f%%',\n",
    "        colors=colors, explode=[0, 0.05, 0.1])\n",
    "ax2.set_title('Effective Class Contribution After Weighting')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Verify class weight proportionality\n",
    "# The product of weight * count should be approximately equal for all classes\n",
    "class_counts = df[target_col].value_counts()\n",
    "\n",
    "print(\"Verification: weight × count should be approximately equal for all classes\")\n",
    "print(\"=\" * 70)\n",
    "products = []\n",
    "for idx, name in enumerate(class_names):\n",
    "    count = class_counts.get(name, 0)\n",
    "    weight = class_weights[idx]\n",
    "    product = weight * count\n",
    "    products.append(product)\n",
    "    print(f\"{name:12s}: {weight:>12.4f} × {count:>10,} = {product:>15.2f}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Mean product: {np.mean(products):,.2f}\")\n",
    "print(f\"Std deviation: {np.std(products):,.2f}\")\n",
    "print(f\"\\n✓ Products are approximately equal, confirming balanced contribution to loss.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focal Loss Explanation\n",
    "\n",
    "In addition to class weighting, we use **Focal Loss** to further address the imbalance:\n",
    "\n",
    "$$FL(p_t) = -\\alpha_t (1 - p_t)^\\gamma \\log(p_t)$$\n",
    "\n",
    "Where:\n",
    "- $p_t$ is the probability of the correct class\n",
    "- $\\gamma$ (gamma) is the focusing parameter (we use $\\gamma = 2$)\n",
    "- $\\alpha_t$ is the class weight\n",
    "\n",
    "**How Focal Loss Helps:**\n",
    "- When $p_t$ is high (easy example), $(1-p_t)^\\gamma$ becomes small, reducing the loss contribution\n",
    "- When $p_t$ is low (hard example), the loss remains high\n",
    "- This focuses training on hard-to-classify examples, which are often minority class samples"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Demonstrate focal loss behavior\n",
    "import numpy as np\n",
    "\n",
    "# Compare cross-entropy vs focal loss\n",
    "p_t = np.linspace(0.01, 0.99, 100)  # Probability of true class\n",
    "\n",
    "# Cross-entropy loss: -log(p_t)\n",
    "ce_loss = -np.log(p_t)\n",
    "\n",
    "# Focal loss with different gamma values\n",
    "gamma_values = [0, 1, 2, 5]\n",
    "focal_losses = {}\n",
    "for gamma in gamma_values:\n",
    "    focal_losses[gamma] = -((1 - p_t) ** gamma) * np.log(p_t)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    label = f'γ={gamma}' + (' (Cross-Entropy)' if gamma == 0 else '')\n",
    "    ax.plot(p_t, focal_losses[gamma], label=label, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Probability of True Class ($p_t$)', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Focal Loss vs Cross-Entropy Loss', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 5])\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotation\n",
    "ax.annotate('Well-classified\\n(low loss)', xy=(0.9, 0.3), fontsize=10,\n",
    "            ha='center', color='green')\n",
    "ax.annotate('Hard examples\\n(high loss)', xy=(0.2, 3), fontsize=10,\n",
    "            ha='center', color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"- With γ=2, well-classified examples (p_t > 0.8) contribute very little to the loss\")\n",
    "print(\"- This allows the model to focus on learning the minority classes\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Data Inspection Summary\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "1. **Dataset Size**: 220,320 samples with 52 sensor features\n",
    "2. **Class Imbalance**: Extreme imbalance with BROKEN class having only 7 samples (0.003%)\n",
    "3. **Missing Values**: Some sensors have missing values that need imputation\n",
    "4. **Feature Correlations**: Several sensors show high correlation, suggesting potential redundancy\n",
    "5. **Data Types**: All sensor features are continuous (float64)\n",
    "\n",
    "**Imbalance Handling Strategy:**\n",
    "- **Class Weights**: NORMAL=0.36, RECOVERING=5.14, BROKEN=10,490.29\n",
    "- **Focal Loss**: γ=2.0 to focus on hard-to-classify examples\n",
    "- **Evaluation Metrics**: Macro F1-score and per-class metrics (not just accuracy)\n",
    "\n",
    "**Next Steps:**\n",
    "- Handle missing values using forward fill imputation\n",
    "- Normalize features using StandardScaler\n",
    "- Create sequences for RNN input\n",
    "- Apply class weighting and focal loss during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Optimization Techniques\n",
    "\n",
    "Training RNNs on imbalanced time-series data requires careful optimization to achieve stable convergence and good generalization. This section explains the optimization techniques we employ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Class Weighting Strategy\n",
    "\n",
    "Class weighting assigns higher importance to minority classes during training by scaling the loss contribution of each sample based on its class.\n",
    "\n",
    "**Formula:** $weight_i = \\frac{N}{n_{classes} \\times count_i}$\n",
    "\n",
    "Where:\n",
    "- $N$ = total number of samples\n",
    "- $n_{classes}$ = number of classes (3)\n",
    "- $count_i$ = number of samples in class $i$\n",
    "\n",
    "**Effect:** The product $weight_i \\times count_i$ becomes approximately equal for all classes, ensuring balanced contribution to the total loss.\n",
    "\n",
    "| Class | Count | Weight | Effective Contribution |\n",
    "|-------|-------|--------|------------------------|\n",
    "| NORMAL | 205,836 | 0.36 | ~73,440 |\n",
    "| RECOVERING | 14,477 | 5.14 | ~73,440 |\n",
    "| BROKEN | 7 | 10,490.29 | ~73,440 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Learning Rate Scheduling (ReduceLROnPlateau)\n",
    "\n",
    "Learning rate scheduling dynamically adjusts the learning rate during training to improve convergence.\n",
    "\n",
    "**ReduceLROnPlateau Strategy:**\n",
    "- Monitor validation loss\n",
    "- When loss stops improving for `patience` epochs, reduce learning rate by `factor`\n",
    "- Continue until `min_lr` is reached\n",
    "\n",
    "**Our Configuration:**\n",
    "```python\n",
    "ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,        # Reduce LR by half\n",
    "    patience=5,        # Wait 5 epochs before reducing\n",
    "    min_lr=1e-6        # Minimum learning rate\n",
    ")\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Starts with larger learning rate for fast initial progress\n",
    "- Reduces learning rate for fine-tuning as training progresses\n",
    "- Helps escape local minima and achieve better convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Gradient Clipping for RNN Stability\n",
    "\n",
    "RNNs are prone to the **exploding gradient problem** where gradients can grow exponentially during backpropagation through time (BPTT).\n",
    "\n",
    "**Gradient Clipping** limits the maximum norm of gradients:\n",
    "\n",
    "$$\\text{if } ||g|| > \\text{threshold}: g \\leftarrow \\frac{g \\times \\text{threshold}}{||g||}$$\n",
    "\n",
    "**Our Configuration:**\n",
    "```python\n",
    "Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Prevents gradient explosion during training\n",
    "- Maintains training stability with long sequences\n",
    "- Allows use of higher learning rates without divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Early Stopping to Prevent Overfitting\n",
    "\n",
    "Early stopping monitors validation performance and stops training when the model begins to overfit.\n",
    "\n",
    "**Our Configuration:**\n",
    "```python\n",
    "EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,           # Stop after 10 epochs without improvement\n",
    "    restore_best_weights=True  # Restore weights from best epoch\n",
    ")\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Prevents overfitting to training data\n",
    "- Saves computation time by stopping unnecessary epochs\n",
    "- Automatically selects the best model checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Optimization Summary\n",
    "\n",
    "| Technique | Purpose | Configuration |\n",
    "|-----------|---------|---------------|\n",
    "| Class Weighting | Handle class imbalance | Inverse frequency weights |\n",
    "| Focal Loss | Focus on hard examples | γ=2.0, α=class weights |\n",
    "| Learning Rate Scheduling | Adaptive learning | ReduceLROnPlateau, factor=0.5 |\n",
    "| Gradient Clipping | Prevent exploding gradients | clipnorm=1.0 |\n",
    "| Early Stopping | Prevent overfitting | patience=10 epochs |\n",
    "| Dropout | Regularization | rate=0.3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Neural Network Model\n",
    "\n",
    "In this section, we define, build, and compile our LSTM-based neural network for multi-class classification of machine status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Why LSTM for Time-Series Classification?\n",
    "\n",
    "**Long Short-Term Memory (LSTM)** networks are a type of Recurrent Neural Network (RNN) specifically designed to learn long-term dependencies in sequential data.\n",
    "\n",
    "#### LSTM Architecture\n",
    "\n",
    "Each LSTM cell contains three gates that control information flow:\n",
    "\n",
    "1. **Forget Gate**: Decides what information to discard from the cell state\n",
    "   - $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n",
    "\n",
    "2. **Input Gate**: Decides what new information to store in the cell state\n",
    "   - $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
    "   - $\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$\n",
    "\n",
    "3. **Output Gate**: Decides what to output based on the cell state\n",
    "   - $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n",
    "   - $h_t = o_t \\times \\tanh(C_t)$\n",
    "\n",
    "#### Why LSTM Over Standard RNN?\n",
    "\n",
    "| Feature | Standard RNN | LSTM |\n",
    "|---------|-------------|------|\n",
    "| Long-term memory | Poor (vanishing gradients) | Excellent (cell state) |\n",
    "| Gradient flow | Degrades over time | Maintained via gates |\n",
    "| Training stability | Difficult | More stable |\n",
    "| Sequence length | Short sequences only | Long sequences supported |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Model Architecture Design\n",
    "\n",
    "Our LSTM model architecture is designed to:\n",
    "1. Process sequences of 60 time steps with 52 sensor features\n",
    "2. Learn hierarchical temporal patterns through stacked LSTM layers\n",
    "3. Prevent overfitting through dropout regularization\n",
    "4. Output probability distributions over 3 classes\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    Input Layer                          │\n",
    "│              Shape: (batch, 60, 52)                     │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "                          │\n",
    "                          ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                   LSTM Layer 1                          │\n",
    "│         128 units, return_sequences=True                │\n",
    "│         Output: (batch, 60, 128)                        │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "                          │\n",
    "                          ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                   Dropout (0.3)                         │\n",
    "│         Randomly drops 30% of connections               │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "                          │\n",
    "                          ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                   LSTM Layer 2                          │\n",
    "│         64 units, return_sequences=False                │\n",
    "│         Output: (batch, 64)                             │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "                          │\n",
    "                          ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                   Dropout (0.3)                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "                          │\n",
    "                          ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                   Dense Layer                           │\n",
    "│         32 units, ReLU activation                       │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "                          │\n",
    "                          ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                   Output Layer                          │\n",
    "│         3 units, Softmax activation                     │\n",
    "│         Output: (batch, 3) - probabilities              │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Hyperparameter Choices\n",
    "\n",
    "| Hyperparameter | Value | Rationale |\n",
    "|----------------|-------|----------|\n",
    "| Sequence Length | 60 | Captures ~1 hour of sensor data (assuming 1-min intervals) |\n",
    "| LSTM Units (Layer 1) | 128 | Sufficient capacity to learn complex patterns |\n",
    "| LSTM Units (Layer 2) | 64 | Reduces dimensionality while preserving key features |\n",
    "| Dropout Rate | 0.3 | Balances regularization without losing too much information |\n",
    "| Dense Units | 32 | Intermediate layer before classification |\n",
    "| Learning Rate | 0.001 | Standard starting point for Adam optimizer |\n",
    "| Batch Size | 64 | Good balance between training speed and gradient stability |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Regularization Techniques\n",
    "\n",
    "#### Dropout Regularization\n",
    "\n",
    "Dropout randomly sets a fraction of input units to 0 during training, which:\n",
    "- Prevents co-adaptation of neurons\n",
    "- Acts as an ensemble of multiple networks\n",
    "- Reduces overfitting on the training data\n",
    "\n",
    "**Our Implementation:**\n",
    "- Dropout rate: 0.3 (30% of neurons dropped)\n",
    "- Applied after each LSTM layer\n",
    "- Only active during training (disabled during inference)\n",
    "\n",
    "#### Why Dropout is Important for This Problem:\n",
    "1. **High-dimensional input**: 52 sensors can lead to overfitting\n",
    "2. **Class imbalance**: Model might memorize minority class examples\n",
    "3. **Limited BROKEN samples**: Only 7 samples make overfitting very likely"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import TensorFlow and model builder\n",
    "import tensorflow as tf\n",
    "from model_builder import build_model, compile_model, get_model_summary\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define model configuration\n",
    "MODEL_CONFIG = {\n",
    "    'seq_length': 60,           # Number of time steps\n",
    "    'n_features': 52,           # Number of sensor features\n",
    "    'lstm_units': [128, 64],    # Units in each LSTM layer\n",
    "    'dropout_rate': 0.3,        # Dropout rate for regularization\n",
    "    'n_classes': 3,             # NORMAL, RECOVERING, BROKEN\n",
    "}\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    'learning_rate': 0.001,\n",
    "    'loss': 'focal',            # Use focal loss for imbalanced data\n",
    "    'focal_loss_gamma': 2.0,\n",
    "    'clipnorm': 1.0,            # Gradient clipping\n",
    "}\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "for key, value in MODEL_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build the LSTM model\n",
    "model = build_model(\n",
    "    seq_length=MODEL_CONFIG['seq_length'],\n",
    "    n_features=MODEL_CONFIG['n_features'],\n",
    "    lstm_units=MODEL_CONFIG['lstm_units'],\n",
    "    dropout_rate=MODEL_CONFIG['dropout_rate'],\n",
    "    n_classes=MODEL_CONFIG['n_classes']\n",
    ")\n",
    "\n",
    "print(\"Model built successfully!\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL ARCHITECTURE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "model.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compile the model with focal loss and gradient clipping\n",
    "# First, compute alpha values from class weights for focal loss\n",
    "alpha_values = compute_alpha_from_weights(class_weights, n_classes=3)\n",
    "print(f\"Focal loss alpha values: {alpha_values}\")\n",
    "\n",
    "model = compile_model(\n",
    "    model=model,\n",
    "    learning_rate=TRAINING_CONFIG['learning_rate'],\n",
    "    loss=TRAINING_CONFIG['loss'],\n",
    "    focal_loss_gamma=TRAINING_CONFIG['focal_loss_gamma'],\n",
    "    focal_loss_alpha=alpha_values,\n",
    "    clipnorm=TRAINING_CONFIG['clipnorm']\n",
    ")\n",
    "\n",
    "print(\"\\nModel compiled successfully!\")\n",
    "print(f\"  Optimizer: Adam (lr={TRAINING_CONFIG['learning_rate']}, clipnorm={TRAINING_CONFIG['clipnorm']})\")\n",
    "print(f\"  Loss: Focal Loss (gamma={TRAINING_CONFIG['focal_loss_gamma']})\")\n",
    "print(f\"  Metrics: accuracy\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Verify model output with a sample input\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample input batch\n",
    "sample_input = np.random.randn(2, MODEL_CONFIG['seq_length'], MODEL_CONFIG['n_features']).astype(np.float32)\n",
    "\n",
    "# Get predictions\n",
    "sample_output = model.predict(sample_input, verbose=0)\n",
    "\n",
    "print(\"Model Output Verification:\")\n",
    "print(f\"  Input shape: {sample_input.shape}\")\n",
    "print(f\"  Output shape: {sample_output.shape}\")\n",
    "print(f\"  Output (probabilities):\")\n",
    "for i, probs in enumerate(sample_output):\n",
    "    print(f\"    Sample {i+1}: NORMAL={probs[0]:.4f}, RECOVERING={probs[1]:.4f}, BROKEN={probs[2]:.4f}\")\n",
    "    print(f\"             Sum={probs.sum():.6f} (should be ~1.0)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Model Architecture Summary\n",
    "\n",
    "**Total Parameters:** ~144,259\n",
    "- LSTM Layer 1: 92,672 parameters\n",
    "- LSTM Layer 2: 49,408 parameters  \n",
    "- Dense Layer: 2,080 parameters\n",
    "- Output Layer: 99 parameters\n",
    "\n",
    "**Key Design Decisions:**\n",
    "\n",
    "1. **Stacked LSTM Layers**: Two LSTM layers allow the model to learn hierarchical temporal features - the first layer captures low-level patterns, the second captures higher-level abstractions.\n",
    "\n",
    "2. **Decreasing Units**: 128 → 64 units creates a bottleneck that forces the model to learn compressed representations.\n",
    "\n",
    "3. **Dropout After Each LSTM**: Prevents overfitting by randomly dropping connections during training.\n",
    "\n",
    "4. **Softmax Output**: Produces valid probability distributions that sum to 1.0, enabling confidence-based predictions.\n",
    "\n",
    "5. **Focal Loss**: Addresses extreme class imbalance by focusing on hard-to-classify examples.\n",
    "\n",
    "6. **Gradient Clipping**: Prevents exploding gradients common in RNN training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industrial Pump Predictive Maintenance using RNN\n",
    "\n",
    "**Course:** 62FIT4ATI - Artificial Intelligence\n",
    "\n",
    "**Topic 2:** Recurrent Neural Network for Predictive Maintenance\n",
    "\n",
    "This notebook is **fully self-contained** - no external .py files required.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install dependencies\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !pip install -q imbalanced-learn\n",
    "else:\n",
    "    print('Running locally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 60)\n",
    "print('Libraries imported!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTIONS (Self-contained - no external imports)\n",
    "# ============================================================\n",
    "\n",
    "def get_feature_columns():\n",
    "    return [f'sensor_{i:02d}' for i in range(52)]\n",
    "\n",
    "def get_target_column():\n",
    "    return 'machine_status'\n",
    "\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    return df\n",
    "\n",
    "def get_dynamic_colors(n):\n",
    "    colors = ['#2ecc71', '#f39c12', '#e74c3c', '#3498db', '#9b59b6']\n",
    "    return colors[:n]\n",
    "\n",
    "print('Helper functions defined!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data - UPDATE PATH FOR COLAB\n",
    "DATA_PATH = 'sensor.csv'  # Change to your path\n",
    "\n",
    "df = load_data(DATA_PATH)\n",
    "feature_cols = get_feature_columns()\n",
    "target_col = get_target_column()\n",
    "\n",
    "print(f'Dataset Shape: {df.shape}')\n",
    "print(f'Total samples: {len(df):,}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution - HANDLES ANY NUMBER OF CLASSES\n",
    "class_counts = df[target_col].value_counts()\n",
    "class_pct = df[target_col].value_counts(normalize=True) * 100\n",
    "n_classes = len(class_counts)\n",
    "\n",
    "print('Class Distribution:')\n",
    "print('=' * 50)\n",
    "for cls in class_counts.index:\n",
    "    print(f'{cls:12s}: {class_counts[cls]:>10,} ({class_pct[cls]:>6.3f}%)')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize - DYNAMIC for any number of classes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = get_dynamic_colors(n_classes)\n",
    "\n",
    "# Bar chart\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(class_counts.index, class_counts.values, color=colors[:len(class_counts)])\n",
    "ax1.set_xlabel('Machine Status')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Class Distribution')\n",
    "if class_counts.max() / class_counts.min() > 10:\n",
    "    ax1.set_yscale('log')\n",
    "for bar, count in zip(bars, class_counts.values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{count:,}', ha='center', va='bottom')\n",
    "\n",
    "# Pie chart - DYNAMIC explode\n",
    "ax2 = axes[1]\n",
    "explode = [0.02 * i for i in range(n_classes)]\n",
    "ax2.pie(class_counts.values, labels=class_counts.index, autopct='%1.2f%%',\n",
    "        colors=colors[:len(class_counts)], explode=explode)\n",
    "ax2.set_title('Class Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "missing = df[feature_cols].isnull().sum()\n",
    "print(f'Total missing values: {missing.sum():,}')\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Handle missing values\ndf[feature_cols] = df[feature_cols].ffill().bfill()\n\n# Fill any remaining NaN with column median (for columns that were all NaN)\nfor col in feature_cols:\n    if df[col].isnull().any():\n        median_val = df[col].median()\n        if pd.isna(median_val):\n            df[col] = 0  # If entire column is NaN, fill with 0\n        else:\n            df[col] = df[col].fillna(median_val)\n\n# Check for constant columns (will cause NaN after scaling)\nconstant_cols = [col for col in feature_cols if df[col].nunique() <= 1]\nif constant_cols:\n    print(f'WARNING: Found {len(constant_cols)} constant columns: {constant_cols[:5]}...')\n    # Add small noise to constant columns to avoid division by zero\n    for col in constant_cols:\n        df[col] = df[col] + np.random.normal(0, 1e-6, len(df))\n\nprint(f'Missing after fill: {df[feature_cols].isnull().sum().sum()}')\nprint(f'Inf values: {np.isinf(df[feature_cols].values).sum()}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels - DYNAMIC based on actual classes in data\n",
    "actual_classes = df[target_col].unique().tolist()\n",
    "print(f'Classes in data: {actual_classes}')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(actual_classes)\n",
    "y_encoded = label_encoder.transform(df[target_col])\n",
    "\n",
    "class_names = list(label_encoder.classes_)\n",
    "n_classes = len(class_names)\n",
    "print(f'Encoded classes: {class_names}')\n",
    "print(f'Number of classes: {n_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights - DYNAMIC\n",
    "class_weights_arr = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weights = dict(enumerate(class_weights_arr))\n",
    "\n",
    "print('Class Weights:')\n",
    "for idx, name in enumerate(class_names):\n",
    "    print(f'  {name}: {class_weights[idx]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare features\nX = df[feature_cols].values\ny = y_encoded\n\n# Train/val/test split with stratify\nmin_class_count = pd.Series(y).value_counts().min()\nuse_stratify = min_class_count >= 2 and n_classes > 1\n\nif use_stratify:\n    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n    X_train_full, X_val, y_train_full, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp)\nelse:\n    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n    X_train_full, X_val, y_train_full, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)\n\nprint(f'Before undersampling:')\nprint(f'  Train: {len(X_train_full):,}, Val: {len(X_val):,}, Test: {len(X_test):,}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Normalize features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_full)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)\n\n# Check for NaN/Inf\ndef check_and_fix_data(data, name):\n    nan_count = np.isnan(data).sum()\n    inf_count = np.isinf(data).sum()\n    if nan_count > 0 or inf_count > 0:\n        print(f'WARNING: {name} has {nan_count} NaN and {inf_count} Inf - fixing...')\n        data = np.nan_to_num(data, nan=0.0, posinf=3.0, neginf=-3.0)\n    return data\n\nX_train_scaled = check_and_fix_data(X_train_scaled, 'X_train')\nX_val_scaled = check_and_fix_data(X_val_scaled, 'X_val')\nX_test_scaled = check_and_fix_data(X_test_scaled, 'X_test')\n\nX_train_scaled = np.clip(X_train_scaled, -10, 10)\nX_val_scaled = np.clip(X_val_scaled, -10, 10)\nX_test_scaled = np.clip(X_test_scaled, -10, 10)\n\nprint(f'Features scaled: range [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create sequences for RNN\nSEQ_LENGTH = 60\n\ndef create_sequences(X, y, seq_length):\n    \"\"\"Create sequences for RNN.\"\"\"\n    n_samples = len(X) - seq_length + 1\n    n_features = X.shape[1]\n    X_seq = np.zeros((n_samples, seq_length, n_features), dtype=np.float32)\n    y_seq = np.zeros(n_samples, dtype=y.dtype)\n    for i in range(n_samples):\n        X_seq[i] = X[i:i + seq_length]\n        y_seq[i] = y[i + seq_length - 1]\n    return X_seq, y_seq\n\nimport gc\nprint('Creating sequences...')\n\nX_train_seq_full, y_train_seq_full = create_sequences(X_train_scaled, y_train_full, SEQ_LENGTH)\nX_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val, SEQ_LENGTH)\nX_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test, SEQ_LENGTH)\n\nprint(f'  Full train sequences: {X_train_seq_full.shape}')\nprint(f'  Val sequences: {X_val_seq.shape}')\nprint(f'  Test sequences: {X_test_seq.shape}')\n\n# Clean up\ndel X_train_scaled, X_val_scaled, X_test_scaled\ngc.collect()"
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# UNDERSAMPLING AT SEQUENCE LEVEL\n# ============================================================\n# Key insight: Undersample AFTER creating sequences to:\n# 1. Preserve temporal patterns (sequences stay intact)\n# 2. Balance classes for meaningful training curves\n# 3. MASSIVELY speed up training (10-20x faster!)\n# ============================================================\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\nprint('=== BEFORE UNDERSAMPLING ===')\nunique, counts = np.unique(y_train_seq_full, return_counts=True)\nfor u, c in zip(unique, counts):\n    print(f'  {class_names[u]:12s}: {c:>8,} ({c/len(y_train_seq_full)*100:.2f}%)')\n\n# Reshape for undersampler (needs 2D)\nn_seq, seq_len, n_feat = X_train_seq_full.shape\nX_train_flat = X_train_seq_full.reshape(n_seq, -1)\n\n# Strategy: Keep all minority, reduce majority to 3x minority\nmin_class_size = min(counts)\nsampling_strategy = {u: min(c, min_class_size * 3) for u, c in zip(unique, counts)}\nprint(f'\\nSampling strategy: {sampling_strategy}')\n\nrus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\nX_train_flat_resampled, y_train_seq = rus.fit_resample(X_train_flat, y_train_seq_full)\n\n# Reshape back to sequences\nX_train_seq = X_train_flat_resampled.reshape(-1, seq_len, n_feat)\n\nprint(f'\\n=== AFTER UNDERSAMPLING ===')\nunique, counts = np.unique(y_train_seq, return_counts=True)\nfor u, c in zip(unique, counts):\n    print(f'  {class_names[u]:12s}: {c:>8,} ({c/len(y_train_seq)*100:.2f}%)')\n\nspeedup = len(X_train_seq_full) / len(X_train_seq)\nprint(f'\\nReduced: {len(X_train_seq_full):,} → {len(X_train_seq):,} sequences')\nprint(f'Training will be ~{speedup:.0f}x FASTER!')\n\n# Clean up\ndel X_train_seq_full, y_train_seq_full, X_train_flat, X_train_flat_resampled\ngc.collect()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# One-hot encode labels\nfrom tensorflow.keras.utils import to_categorical\n\ny_train_cat = to_categorical(y_train_seq, num_classes=n_classes)\ny_val_cat = to_categorical(y_val_seq, num_classes=n_classes)\ny_test_cat = to_categorical(y_test_seq, num_classes=n_classes)\n\n# Convert to float32 for compatibility\nX_train_seq = X_train_seq.astype(np.float32)\nX_val_seq = X_val_seq.astype(np.float32)\nX_test_seq = X_test_seq.astype(np.float32)\ny_train_cat = y_train_cat.astype(np.float32)\ny_val_cat = y_val_cat.astype(np.float32)\ny_test_cat = y_test_cat.astype(np.float32)\n\nprint(f'One-hot shapes: {y_train_cat.shape}')\nprint(f'Data type: X={X_train_seq.dtype}, y={y_train_cat.dtype}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "print(f'TensorFlow: {tf.__version__}')\n",
    "print(f'GPU: {len(tf.config.list_physical_devices(\"GPU\")) > 0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# VALIDATION: Check data before training\nprint('=== DATA VALIDATION ===')\nprint(f'X_train_seq - NaN: {np.isnan(X_train_seq).sum()}, Inf: {np.isinf(X_train_seq).sum()}')\nprint(f'y_train_cat - NaN: {np.isnan(y_train_cat).sum()}, Inf: {np.isinf(y_train_cat).sum()}')\nprint(f'X_train_seq range: [{X_train_seq.min():.2f}, {X_train_seq.max():.2f}]')\n\n# Check class distribution in training set (after undersampling)\nprint('\\n=== CLASS DISTRIBUTION (After Undersampling) ===')\nunique, counts = np.unique(y_train_seq, return_counts=True)\nfor u, c in zip(unique, counts):\n    print(f'  {class_names[u]}: {c:,} ({c/len(y_train_seq)*100:.1f}%)')\n\n# ============================================================\n# OPTIMIZATION TECHNIQUES SUMMARY\n# ============================================================\nprint('\\n' + '='*60)\nprint('OPTIMIZATION TECHNIQUES FOR CLASS IMBALANCE & STABILITY')\nprint('='*60)\nprint('''\nTECHNIQUE 1: UNDERSAMPLING (Data-Level)\n  - Reduces majority class to balance training data\n  - Forces model to learn ALL classes, not just majority\n  - Speeds up training significantly\n\nTECHNIQUE 2: CLASS WEIGHTS (Algorithm-Level)  \n  - Penalizes misclassification of minority classes more\n  - Works alongside undersampling for better results\n\nTECHNIQUE 3: GRADIENT CLIPPING (Stability)\n  - Prevents exploding gradients during training\n  - Essential for LSTM with imbalanced data\n\nTECHNIQUE 4: LEARNING RATE SCHEDULING (Optimization)\n  - ReduceLROnPlateau adapts learning rate during training\n  - Helps escape local minima\n''')\nprint('='*60)\n\nn_features = len(feature_cols)\n\n# Build 2-layer LSTM model\nmodel = Sequential([\n    LSTM(128, return_sequences=True, input_shape=(SEQ_LENGTH, n_features)),\n    Dropout(0.3),\n    LSTM(64, return_sequences=False),\n    Dropout(0.3),\n    Dense(32, activation='relu'),\n    Dense(n_classes, activation='softmax')\n])\n\n# Class weights (still useful even with undersampling)\nMAX_WEIGHT = 5.0  # Lower since we already undersampled\nclass_weights_capped = {k: min(v, MAX_WEIGHT) for k, v in class_weights.items()}\n\nmodel.compile(\n    optimizer=Adam(learning_rate=0.001, clipnorm=1.0),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Callbacks for training optimization\nimport os\nos.makedirs('models', exist_ok=True)\n\ncallbacks = [\n    EarlyStopping(\n        monitor='val_loss', \n        patience=8,  # More patience for undersampled data\n        restore_best_weights=True, \n        verbose=1\n    ),\n    ReduceLROnPlateau(\n        monitor='val_loss', \n        factor=0.5, \n        patience=4, \n        min_lr=1e-6, \n        verbose=1\n    ),\n    ModelCheckpoint(\n        'models/best_model.keras', \n        monitor='val_loss', \n        save_best_only=True, \n        verbose=0\n    )\n]\n\nprint('Callbacks configured:')\nprint('  - EarlyStopping: patience=8')\nprint('  - ReduceLROnPlateau: factor=0.5, patience=4')\nprint('  - ModelCheckpoint: saves best model')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train model with undersampled data\n# Much faster now due to reduced dataset size!\nBATCH_SIZE = 64  # Smaller batch for smaller dataset\n\nsteps_per_epoch = len(X_train_seq) // BATCH_SIZE\nprint(f'Training samples: {len(X_train_seq):,}')\nprint(f'Batch size: {BATCH_SIZE}')\nprint(f'Steps per epoch: {steps_per_epoch}')\nprint(f'\\nExpected time: ~{steps_per_epoch * 0.5:.0f}s per epoch (vs ~300s before!)')\n\nhistory = model.fit(\n    X_train_seq, y_train_cat,\n    validation_data=(X_val_seq, y_val_cat),\n    epochs=30,\n    batch_size=BATCH_SIZE,\n    class_weight=class_weights_capped,\n    callbacks=callbacks,\n    verbose=1\n)\n\nprint('\\nTraining complete!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history.history['loss'], label='Train')\n",
    "axes[0].plot(history.history['val_loss'], label='Val')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history.history['accuracy'], label='Train')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Val')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_proba = model.predict(X_test_seq)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_true = y_test_seq\n",
    "\n",
    "print(f'Predictions: {len(y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# EVALUATION METRICS FOR IMBALANCED DATA\n# ============================================================\n# Accuracy alone is misleading for imbalanced datasets!\n# Key metrics: Precision, Recall, F1-Score (especially macro/weighted)\n# ============================================================\n\nfrom sklearn.metrics import f1_score, precision_score, recall_score, balanced_accuracy_score\n\nprint('='*60)\nprint('METRICS FOR IMBALANCED DATA')\nprint('='*60)\n\n# Calculate key metrics\naccuracy = (y_pred == y_true).mean()\nbalanced_acc = balanced_accuracy_score(y_true, y_pred)\nf1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\nprecision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\nrecall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n\nprint(f'\\nOverall Metrics:')\nprint(f'  Accuracy:          {accuracy:.4f}  (misleading for imbalanced data!)')\nprint(f'  Balanced Accuracy: {balanced_acc:.4f}  (accounts for class imbalance)')\nprint(f'  F1 Macro:          {f1_macro:.4f}  (equal weight to all classes)')\nprint(f'  F1 Weighted:       {f1_weighted:.4f}  (weighted by class frequency)')\nprint(f'  Precision Macro:   {precision_macro:.4f}')\nprint(f'  Recall Macro:      {recall_macro:.4f}  (most important for failure detection!)')\n\nprint('\\n' + '='*60)\nprint('Classification Report (Per-Class Metrics):')\nprint('='*60)\nprint(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))\n\n# Highlight minority class performance\nprint('='*60)\nprint('MINORITY CLASS ANALYSIS (Critical for Predictive Maintenance)')\nprint('='*60)\nfor i, name in enumerate(class_names):\n    class_mask = y_true == i\n    class_count = class_mask.sum()\n    if class_count > 0:\n        class_recall = (y_pred[class_mask] == i).sum() / class_count\n        class_pred_count = (y_pred == i).sum()\n        print(f'{name:12s}: {class_count:>6,} samples, Recall={class_recall:.2%}, Predicted={class_pred_count:,}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix - DYNAMIC\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confusion matrix\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Normalized Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model artifacts\n",
    "import pickle\n",
    "\n",
    "model.save('models/final_model.keras')\n",
    "\n",
    "with open('models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open('models/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print('Model and artifacts saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def predict_status(sensor_data, model, scaler, label_encoder, seq_length=60):\n",
    "    \"\"\"\n",
    "    Predict machine status from sensor data.\n",
    "    sensor_data: array of shape (seq_length, n_features) or (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < seq_length:\n",
    "        raise ValueError(f'Need at least {seq_length} samples')\n",
    "    \n",
    "    # Take last seq_length samples\n",
    "    data = sensor_data[-seq_length:]\n",
    "    \n",
    "    # Scale\n",
    "    data_scaled = scaler.transform(data)\n",
    "    \n",
    "    # Reshape for model\n",
    "    data_seq = data_scaled.reshape(1, seq_length, -1)\n",
    "    \n",
    "    # Predict\n",
    "    proba = model.predict(data_seq, verbose=0)[0]\n",
    "    pred_idx = np.argmax(proba)\n",
    "    pred_label = label_encoder.inverse_transform([pred_idx])[0]\n",
    "    \n",
    "    return {\n",
    "        'prediction': pred_label,\n",
    "        'confidence': float(proba[pred_idx]),\n",
    "        'probabilities': {label_encoder.classes_[i]: float(proba[i]) for i in range(len(proba))}\n",
    "    }\n",
    "\n",
    "print('Inference function ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "sample_idx = 1000\n",
    "sample_data = df[feature_cols].iloc[sample_idx:sample_idx + SEQ_LENGTH].values\n",
    "actual = df[target_col].iloc[sample_idx + SEQ_LENGTH - 1]\n",
    "\n",
    "result = predict_status(sample_data, model, scaler, label_encoder, SEQ_LENGTH)\n",
    "\n",
    "print(f'Actual: {actual}')\n",
    "print(f'Predicted: {result[\"prediction\"]}')\n",
    "print(f'Confidence: {result[\"confidence\"]:.2%}')\n",
    "print(f'Probabilities: {result[\"probabilities\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## Section 6: Conclusion\n\n### Model Architecture\n- **2-layer LSTM** (128 → 64 units) for temporal pattern recognition\n- Sequence length: 60 timesteps\n- 52 sensor features\n\n### Optimization Techniques Applied\n\n| # | Technique | Category | Purpose | Implementation |\n|---|-----------|----------|---------|----------------|\n| 1 | **Undersampling** | Data-Level | Balance classes, speed up training | RandomUnderSampler (3:1 ratio) |\n| 2 | **Class Weights** | Algorithm-Level | Penalize minority misclassification | Balanced weights, capped at 5.0 |\n| 3 | **Gradient Clipping** | Training Stability | Prevent exploding gradients | `clipnorm=1.0` in Adam |\n| 4 | **LR Scheduling** | Optimization | Adaptive learning rate | ReduceLROnPlateau |\n\n### Why These Techniques?\n\n**The Challenge:** Extreme class imbalance (NORMAL ~98%, BROKEN/RECOVERING ~2%)\n- Without techniques: Model predicts \"NORMAL\" for everything → 98% accuracy but 0% failure detection\n- With techniques: Model learns to detect minority classes → Lower accuracy but meaningful predictions\n\n### Evaluation Metrics for Imbalanced Data\n\n| Metric | Why It Matters |\n|--------|----------------|\n| **Balanced Accuracy** | Accounts for class imbalance, gives true performance |\n| **F1-Score (Macro)** | Equal weight to all classes |\n| **Recall (per class)** | Critical - can we detect failures? |\n\n### Key Insight\n> **Accuracy is misleading for imbalanced data!** A model with 98% accuracy might have 0% recall on the classes we care about (BROKEN, RECOVERING). Always evaluate with balanced metrics."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
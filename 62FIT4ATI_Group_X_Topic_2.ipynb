{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industrial Pump Predictive Maintenance using RNN\n",
    "\n",
    "**Course:** 62FIT4ATI - Artificial Intelligence\n",
    "\n",
    "**Topic 2:** Recurrent Neural Network for Predictive Maintenance\n",
    "\n",
    "This notebook is **fully self-contained** - no external .py files required.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install dependencies\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !pip install -q imbalanced-learn\n",
    "else:\n",
    "    print('Running locally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 60)\n",
    "print('Libraries imported!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTIONS (Self-contained - no external imports)\n",
    "# ============================================================\n",
    "\n",
    "def get_feature_columns():\n",
    "    return [f'sensor_{i:02d}' for i in range(52)]\n",
    "\n",
    "def get_target_column():\n",
    "    return 'machine_status'\n",
    "\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    return df\n",
    "\n",
    "def get_dynamic_colors(n):\n",
    "    colors = ['#2ecc71', '#f39c12', '#e74c3c', '#3498db', '#9b59b6']\n",
    "    return colors[:n]\n",
    "\n",
    "print('Helper functions defined!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data - UPDATE PATH FOR COLAB\n",
    "DATA_PATH = 'sensor.csv'  # Change to your path\n",
    "\n",
    "df = load_data(DATA_PATH)\n",
    "feature_cols = get_feature_columns()\n",
    "target_col = get_target_column()\n",
    "\n",
    "print(f'Dataset Shape: {df.shape}')\n",
    "print(f'Total samples: {len(df):,}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution - HANDLES ANY NUMBER OF CLASSES\n",
    "class_counts = df[target_col].value_counts()\n",
    "class_pct = df[target_col].value_counts(normalize=True) * 100\n",
    "n_classes = len(class_counts)\n",
    "\n",
    "print('Class Distribution:')\n",
    "print('=' * 50)\n",
    "for cls in class_counts.index:\n",
    "    print(f'{cls:12s}: {class_counts[cls]:>10,} ({class_pct[cls]:>6.3f}%)')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize - DYNAMIC for any number of classes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = get_dynamic_colors(n_classes)\n",
    "\n",
    "# Bar chart\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(class_counts.index, class_counts.values, color=colors[:len(class_counts)])\n",
    "ax1.set_xlabel('Machine Status')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Class Distribution')\n",
    "if class_counts.max() / class_counts.min() > 10:\n",
    "    ax1.set_yscale('log')\n",
    "for bar, count in zip(bars, class_counts.values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{count:,}', ha='center', va='bottom')\n",
    "\n",
    "# Pie chart - DYNAMIC explode\n",
    "ax2 = axes[1]\n",
    "explode = [0.02 * i for i in range(n_classes)]\n",
    "ax2.pie(class_counts.values, labels=class_counts.index, autopct='%1.2f%%',\n",
    "        colors=colors[:len(class_counts)], explode=explode)\n",
    "ax2.set_title('Class Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "missing = df[feature_cols].isnull().sum()\n",
    "print(f'Total missing values: {missing.sum():,}')\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "df[feature_cols] = df[feature_cols].ffill().bfill()\n",
    "print(f'Missing after fill: {df[feature_cols].isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels - DYNAMIC based on actual classes in data\n",
    "actual_classes = df[target_col].unique().tolist()\n",
    "print(f'Classes in data: {actual_classes}')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(actual_classes)\n",
    "y_encoded = label_encoder.transform(df[target_col])\n",
    "\n",
    "class_names = list(label_encoder.classes_)\n",
    "n_classes = len(class_names)\n",
    "print(f'Encoded classes: {class_names}')\n",
    "print(f'Number of classes: {n_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights - DYNAMIC\n",
    "class_weights_arr = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weights = dict(enumerate(class_weights_arr))\n",
    "\n",
    "print('Class Weights:')\n",
    "for idx, name in enumerate(class_names):\n",
    "    print(f'  {name}: {class_weights[idx]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "X = df[feature_cols].values\n",
    "y = y_encoded\n",
    "\n",
    "# Train/val/test split - HANDLES ANY NUMBER OF CLASSES (even 1)\n",
    "# Only use stratify if we have enough samples per class\n",
    "min_class_count = pd.Series(y).value_counts().min()\n",
    "use_stratify = min_class_count >= 2 and n_classes > 1\n",
    "\n",
    "if use_stratify:\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp)\n",
    "else:\n",
    "    # No stratify for single class or very few samples\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)\n",
    "    print(f'Note: Stratified split disabled (n_classes={n_classes}, min_samples={min_class_count})')\n",
    "\n",
    "print(f'Train: {len(X_train):,}, Val: {len(X_val):,}, Test: {len(X_test):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print('Features normalized!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for RNN\n",
    "SEQ_LENGTH = 60\n",
    "\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_length + 1):\n",
    "        X_seq.append(X[i:i + seq_length])\n",
    "        y_seq.append(y[i + seq_length - 1])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, SEQ_LENGTH)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val, SEQ_LENGTH)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test, SEQ_LENGTH)\n",
    "\n",
    "print(f'Sequence shapes:')\n",
    "print(f'  X_train: {X_train_seq.shape}')\n",
    "print(f'  X_val: {X_val_seq.shape}')\n",
    "print(f'  X_test: {X_test_seq.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train_seq, num_classes=n_classes)\n",
    "y_val_cat = to_categorical(y_val_seq, num_classes=n_classes)\n",
    "y_test_cat = to_categorical(y_test_seq, num_classes=n_classes)\n",
    "\n",
    "print(f'One-hot shapes: {y_train_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "print(f'TensorFlow: {tf.__version__}')\n",
    "print(f'GPU: {len(tf.config.list_physical_devices(\"GPU\")) > 0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal Loss for imbalanced data\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def focal_loss(gamma=2.0, alpha=None):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "        weight = K.pow(1 - y_pred, gamma) * y_true\n",
    "        focal = weight * cross_entropy\n",
    "        if alpha is not None:\n",
    "            focal = focal * alpha\n",
    "        return K.sum(focal, axis=-1)\n",
    "    return focal_loss_fixed\n",
    "\n",
    "print('Focal loss defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model - DYNAMIC n_classes\n",
    "n_features = len(feature_cols)\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(SEQ_LENGTH, n_features)),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(n_classes, activation='softmax')  # DYNAMIC\n",
    "])\n",
    "\n",
    "# Compile with focal loss\n",
    "alpha = np.array([class_weights[i] for i in range(n_classes)])\n",
    "alpha = alpha / alpha.sum()  # Normalize\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "    loss=focal_loss(gamma=2.0, alpha=alpha),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),\n",
    "    ModelCheckpoint('models/best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "print('Callbacks ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_cat,\n",
    "    validation_data=(X_val_seq, y_val_cat),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history.history['loss'], label='Train')\n",
    "axes[0].plot(history.history['val_loss'], label='Val')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history.history['accuracy'], label='Train')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Val')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_proba = model.predict(X_test_seq)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_true = y_test_seq\n",
    "\n",
    "print(f'Predictions: {len(y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report - DYNAMIC class names\n",
    "print('Classification Report:')\n",
    "print('=' * 60)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix - DYNAMIC\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confusion matrix\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Normalized Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model artifacts\n",
    "import pickle\n",
    "\n",
    "model.save('models/final_model.keras')\n",
    "\n",
    "with open('models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open('models/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print('Model and artifacts saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def predict_status(sensor_data, model, scaler, label_encoder, seq_length=60):\n",
    "    \"\"\"\n",
    "    Predict machine status from sensor data.\n",
    "    sensor_data: array of shape (seq_length, n_features) or (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < seq_length:\n",
    "        raise ValueError(f'Need at least {seq_length} samples')\n",
    "    \n",
    "    # Take last seq_length samples\n",
    "    data = sensor_data[-seq_length:]\n",
    "    \n",
    "    # Scale\n",
    "    data_scaled = scaler.transform(data)\n",
    "    \n",
    "    # Reshape for model\n",
    "    data_seq = data_scaled.reshape(1, seq_length, -1)\n",
    "    \n",
    "    # Predict\n",
    "    proba = model.predict(data_seq, verbose=0)[0]\n",
    "    pred_idx = np.argmax(proba)\n",
    "    pred_label = label_encoder.inverse_transform([pred_idx])[0]\n",
    "    \n",
    "    return {\n",
    "        'prediction': pred_label,\n",
    "        'confidence': float(proba[pred_idx]),\n",
    "        'probabilities': {label_encoder.classes_[i]: float(proba[i]) for i in range(len(proba))}\n",
    "    }\n",
    "\n",
    "print('Inference function ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "sample_idx = 1000\n",
    "sample_data = df[feature_cols].iloc[sample_idx:sample_idx + SEQ_LENGTH].values\n",
    "actual = df[target_col].iloc[sample_idx + SEQ_LENGTH - 1]\n",
    "\n",
    "result = predict_status(sample_data, model, scaler, label_encoder, SEQ_LENGTH)\n",
    "\n",
    "print(f'Actual: {actual}')\n",
    "print(f'Predicted: {result[\"prediction\"]}')\n",
    "print(f'Confidence: {result[\"confidence\"]:.2%}')\n",
    "print(f'Probabilities: {result[\"probabilities\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Loading and exploring sensor data\n",
    "2. Handling class imbalance with class weights and focal loss\n",
    "3. Building an LSTM model for time-series classification\n",
    "4. Evaluating model performance\n",
    "5. Creating an inference pipeline\n",
    "\n",
    "The model dynamically handles any number of classes present in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
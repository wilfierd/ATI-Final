{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industrial Pump Predictive Maintenance using RNN\n",
    "\n",
    "**Course:** 62FIT4ATI - Artificial Intelligence\n",
    "\n",
    "**Topic 2:** Recurrent Neural Network for Predictive Maintenance\n",
    "\n",
    "This notebook is **fully self-contained** - no external .py files required.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install dependencies\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !pip install -q imbalanced-learn\n",
    "else:\n",
    "    print('Running locally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 60)\n",
    "print('Libraries imported!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTIONS (Self-contained - no external imports)\n",
    "# ============================================================\n",
    "\n",
    "def get_feature_columns():\n",
    "    return [f'sensor_{i:02d}' for i in range(52)]\n",
    "\n",
    "def get_target_column():\n",
    "    return 'machine_status'\n",
    "\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    return df\n",
    "\n",
    "def get_dynamic_colors(n):\n",
    "    colors = ['#2ecc71', '#f39c12', '#e74c3c', '#3498db', '#9b59b6']\n",
    "    return colors[:n]\n",
    "\n",
    "print('Helper functions defined!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data - UPDATE PATH FOR COLAB\n",
    "DATA_PATH = 'sensor.csv'  # Change to your path\n",
    "\n",
    "df = load_data(DATA_PATH)\n",
    "feature_cols = get_feature_columns()\n",
    "target_col = get_target_column()\n",
    "\n",
    "print(f'Dataset Shape: {df.shape}')\n",
    "print(f'Total samples: {len(df):,}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution - HANDLES ANY NUMBER OF CLASSES\n",
    "class_counts = df[target_col].value_counts()\n",
    "class_pct = df[target_col].value_counts(normalize=True) * 100\n",
    "n_classes = len(class_counts)\n",
    "\n",
    "print('Class Distribution:')\n",
    "print('=' * 50)\n",
    "for cls in class_counts.index:\n",
    "    print(f'{cls:12s}: {class_counts[cls]:>10,} ({class_pct[cls]:>6.3f}%)')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize - DYNAMIC for any number of classes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = get_dynamic_colors(n_classes)\n",
    "\n",
    "# Bar chart\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(class_counts.index, class_counts.values, color=colors[:len(class_counts)])\n",
    "ax1.set_xlabel('Machine Status')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Class Distribution')\n",
    "if class_counts.max() / class_counts.min() > 10:\n",
    "    ax1.set_yscale('log')\n",
    "for bar, count in zip(bars, class_counts.values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{count:,}', ha='center', va='bottom')\n",
    "\n",
    "# Pie chart - DYNAMIC explode\n",
    "ax2 = axes[1]\n",
    "explode = [0.02 * i for i in range(n_classes)]\n",
    "ax2.pie(class_counts.values, labels=class_counts.index, autopct='%1.2f%%',\n",
    "        colors=colors[:len(class_counts)], explode=explode)\n",
    "ax2.set_title('Class Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "missing = df[feature_cols].isnull().sum()\n",
    "print(f'Total missing values: {missing.sum():,}')\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Handle missing values\ndf[feature_cols] = df[feature_cols].ffill().bfill()\n\n# Fill any remaining NaN with column median (for columns that were all NaN)\nfor col in feature_cols:\n    if df[col].isnull().any():\n        median_val = df[col].median()\n        if pd.isna(median_val):\n            df[col] = 0  # If entire column is NaN, fill with 0\n        else:\n            df[col] = df[col].fillna(median_val)\n\n# Check for constant columns (will cause NaN after scaling)\nconstant_cols = [col for col in feature_cols if df[col].nunique() <= 1]\nif constant_cols:\n    print(f'WARNING: Found {len(constant_cols)} constant columns: {constant_cols[:5]}...')\n    # Add small noise to constant columns to avoid division by zero\n    for col in constant_cols:\n        df[col] = df[col] + np.random.normal(0, 1e-6, len(df))\n\nprint(f'Missing after fill: {df[feature_cols].isnull().sum().sum()}')\nprint(f'Inf values: {np.isinf(df[feature_cols].values).sum()}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels - DYNAMIC based on actual classes in data\n",
    "actual_classes = df[target_col].unique().tolist()\n",
    "print(f'Classes in data: {actual_classes}')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(actual_classes)\n",
    "y_encoded = label_encoder.transform(df[target_col])\n",
    "\n",
    "class_names = list(label_encoder.classes_)\n",
    "n_classes = len(class_names)\n",
    "print(f'Encoded classes: {class_names}')\n",
    "print(f'Number of classes: {n_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights - DYNAMIC\n",
    "class_weights_arr = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weights = dict(enumerate(class_weights_arr))\n",
    "\n",
    "print('Class Weights:')\n",
    "for idx, name in enumerate(class_names):\n",
    "    print(f'  {name}: {class_weights[idx]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "X = df[feature_cols].values\n",
    "y = y_encoded\n",
    "\n",
    "# Train/val/test split - HANDLES ANY NUMBER OF CLASSES (even 1)\n",
    "# Only use stratify if we have enough samples per class\n",
    "min_class_count = pd.Series(y).value_counts().min()\n",
    "use_stratify = min_class_count >= 2 and n_classes > 1\n",
    "\n",
    "if use_stratify:\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp)\n",
    "else:\n",
    "    # No stratify for single class or very few samples\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)\n",
    "    print(f'Note: Stratified split disabled (n_classes={n_classes}, min_samples={min_class_count})')\n",
    "\n",
    "print(f'Train: {len(X_train):,}, Val: {len(X_val):,}, Test: {len(X_test):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Normalize features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)\n\n# CRITICAL: Check for NaN/Inf after scaling\ndef check_and_fix_data(data, name):\n    nan_count = np.isnan(data).sum()\n    inf_count = np.isinf(data).sum()\n    if nan_count > 0 or inf_count > 0:\n        print(f'WARNING: {name} has {nan_count} NaN and {inf_count} Inf values - fixing...')\n        data = np.nan_to_num(data, nan=0.0, posinf=3.0, neginf=-3.0)\n    return data\n\nX_train_scaled = check_and_fix_data(X_train_scaled, 'X_train_scaled')\nX_val_scaled = check_and_fix_data(X_val_scaled, 'X_val_scaled')\nX_test_scaled = check_and_fix_data(X_test_scaled, 'X_test_scaled')\n\n# Clip extreme values to prevent overflow during training\nX_train_scaled = np.clip(X_train_scaled, -10, 10)\nX_val_scaled = np.clip(X_val_scaled, -10, 10)\nX_test_scaled = np.clip(X_test_scaled, -10, 10)\n\nprint('Features normalized!')\nprint(f'X_train_scaled - min: {X_train_scaled.min():.2f}, max: {X_train_scaled.max():.2f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create sequences for RNN - MEMORY OPTIMIZED for Colab\nSEQ_LENGTH = 60\n\ndef create_sequences(X, y, seq_length):\n    \"\"\"Memory-efficient sequence creation.\"\"\"\n    n_samples = len(X) - seq_length + 1\n    n_features = X.shape[1]\n    \n    # Pre-allocate array (more memory efficient than list append)\n    X_seq = np.zeros((n_samples, seq_length, n_features), dtype=np.float32)\n    y_seq = np.zeros(n_samples, dtype=y.dtype)\n    \n    for i in range(n_samples):\n        X_seq[i] = X[i:i + seq_length]\n        y_seq[i] = y[i + seq_length - 1]\n    \n    return X_seq, y_seq\n\nimport gc\nprint('Creating sequences (may take a moment)...')\n\nX_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, SEQ_LENGTH)\ngc.collect()  # Free memory\nprint(f'  Train done: {X_train_seq.shape}')\n\nX_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val, SEQ_LENGTH)\ngc.collect()\nprint(f'  Val done: {X_val_seq.shape}')\n\nX_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test, SEQ_LENGTH)\ngc.collect()\nprint(f'  Test done: {X_test_seq.shape}')\n\n# Clean up original arrays to free memory\ndel X_train_scaled, X_val_scaled, X_test_scaled\ngc.collect()\n\nprint(f'Memory used: ~{(X_train_seq.nbytes + X_val_seq.nbytes + X_test_seq.nbytes) / 1e9:.2f} GB')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# One-hot encode labels\nfrom tensorflow.keras.utils import to_categorical\n\ny_train_cat = to_categorical(y_train_seq, num_classes=n_classes)\ny_val_cat = to_categorical(y_val_seq, num_classes=n_classes)\ny_test_cat = to_categorical(y_test_seq, num_classes=n_classes)\n\n# Convert to float32 for compatibility\nX_train_seq = X_train_seq.astype(np.float32)\nX_val_seq = X_val_seq.astype(np.float32)\nX_test_seq = X_test_seq.astype(np.float32)\ny_train_cat = y_train_cat.astype(np.float32)\ny_val_cat = y_val_cat.astype(np.float32)\ny_test_cat = y_test_cat.astype(np.float32)\n\nprint(f'One-hot shapes: {y_train_cat.shape}')\nprint(f'Data type: X={X_train_seq.dtype}, y={y_train_cat.dtype}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "print(f'TensorFlow: {tf.__version__}')\n",
    "print(f'GPU: {len(tf.config.list_physical_devices(\"GPU\")) > 0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use standard categorical crossentropy - more stable for extreme imbalance\n",
    "# Class weights will handle the imbalance instead of focal loss\n",
    "print('Using categorical crossentropy with class weights (more stable than focal loss)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# VALIDATION: Check data before training\nprint('=== DATA VALIDATION ===')\nprint(f'X_train_seq - NaN: {np.isnan(X_train_seq).sum()}, Inf: {np.isinf(X_train_seq).sum()}')\nprint(f'y_train_cat - NaN: {np.isnan(y_train_cat).sum()}, Inf: {np.isinf(y_train_cat).sum()}')\nprint(f'X_train_seq range: [{X_train_seq.min():.2f}, {X_train_seq.max():.2f}]')\n\n# Build model - DYNAMIC n_classes\nn_features = len(feature_cols)\n\nmodel = Sequential([\n    LSTM(128, return_sequences=True, input_shape=(SEQ_LENGTH, n_features)),\n    Dropout(0.3),\n    LSTM(64, return_sequences=False),\n    Dropout(0.3),\n    Dense(32, activation='relu'),\n    Dense(n_classes, activation='softmax')  # DYNAMIC\n])\n\n# Cap class weights more aggressively to prevent NaN\nMAX_WEIGHT = 10.0  # Reduced from 50 to be safer\nclass_weights_capped = {k: min(v, MAX_WEIGHT) for k, v in class_weights.items()}\nprint(f'Original weights: {class_weights}')\nprint(f'Capped weights (max={MAX_WEIGHT}): {class_weights_capped}')\n\n# Use lower learning rate for stability\nmodel.compile(\n    optimizer=Adam(learning_rate=0.0005, clipnorm=1.0),  # Reduced from 0.001\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),\n",
    "    ModelCheckpoint('models/best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "print('Callbacks ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with CAPPED weights\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_cat,\n",
    "    validation_data=(X_val_seq, y_val_cat),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weights_capped,  # Use capped weights!\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history.history['loss'], label='Train')\n",
    "axes[0].plot(history.history['val_loss'], label='Val')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history.history['accuracy'], label='Train')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Val')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_proba = model.predict(X_test_seq)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_true = y_test_seq\n",
    "\n",
    "print(f'Predictions: {len(y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report - DYNAMIC class names\n",
    "print('Classification Report:')\n",
    "print('=' * 60)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix - DYNAMIC\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confusion matrix\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Normalized Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model artifacts\n",
    "import pickle\n",
    "\n",
    "model.save('models/final_model.keras')\n",
    "\n",
    "with open('models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open('models/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print('Model and artifacts saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def predict_status(sensor_data, model, scaler, label_encoder, seq_length=60):\n",
    "    \"\"\"\n",
    "    Predict machine status from sensor data.\n",
    "    sensor_data: array of shape (seq_length, n_features) or (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < seq_length:\n",
    "        raise ValueError(f'Need at least {seq_length} samples')\n",
    "    \n",
    "    # Take last seq_length samples\n",
    "    data = sensor_data[-seq_length:]\n",
    "    \n",
    "    # Scale\n",
    "    data_scaled = scaler.transform(data)\n",
    "    \n",
    "    # Reshape for model\n",
    "    data_seq = data_scaled.reshape(1, seq_length, -1)\n",
    "    \n",
    "    # Predict\n",
    "    proba = model.predict(data_seq, verbose=0)[0]\n",
    "    pred_idx = np.argmax(proba)\n",
    "    pred_label = label_encoder.inverse_transform([pred_idx])[0]\n",
    "    \n",
    "    return {\n",
    "        'prediction': pred_label,\n",
    "        'confidence': float(proba[pred_idx]),\n",
    "        'probabilities': {label_encoder.classes_[i]: float(proba[i]) for i in range(len(proba))}\n",
    "    }\n",
    "\n",
    "print('Inference function ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "sample_idx = 1000\n",
    "sample_data = df[feature_cols].iloc[sample_idx:sample_idx + SEQ_LENGTH].values\n",
    "actual = df[target_col].iloc[sample_idx + SEQ_LENGTH - 1]\n",
    "\n",
    "result = predict_status(sample_data, model, scaler, label_encoder, SEQ_LENGTH)\n",
    "\n",
    "print(f'Actual: {actual}')\n",
    "print(f'Predicted: {result[\"prediction\"]}')\n",
    "print(f'Confidence: {result[\"confidence\"]:.2%}')\n",
    "print(f'Probabilities: {result[\"probabilities\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Loading and exploring sensor data\n",
    "2. Handling class imbalance with class weights and focal loss\n",
    "3. Building an LSTM model for time-series classification\n",
    "4. Evaluating model performance\n",
    "5. Creating an inference pipeline\n",
    "\n",
    "The model dynamically handles any number of classes present in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}